{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80df2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.util import filter_spans\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from ner_service import run_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2014cd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>ner_tags_str</th>\n",
       "      <th>entities_pred</th>\n",
       "      <th>llm_time</th>\n",
       "      <th>llm_prompt_tokens</th>\n",
       "      <th>llm_completion_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7567</td>\n",
       "      <td>Egyptian government newspapers have criticised...</td>\n",
       "      <td>B-MISC O O O O B-LOC O O B-MISC O O O O O O O ...</td>\n",
       "      <td>[{'entity': 'Egyptian government', 'label': 'O...</td>\n",
       "      <td>0.844970</td>\n",
       "      <td>203</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>624</td>\n",
       "      <td>Coritiba 1 Atletico Mineiro 0</td>\n",
       "      <td>B-ORG O B-ORG I-ORG O</td>\n",
       "      <td>[{'entity': 'Coritiba', 'label': 'ORG'}, {'ent...</td>\n",
       "      <td>0.696006</td>\n",
       "      <td>171</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    id                                      sentence_text  \\\n",
       "0           0  7567  Egyptian government newspapers have criticised...   \n",
       "1           1   624                      Coritiba 1 Atletico Mineiro 0   \n",
       "\n",
       "                                        ner_tags_str  \\\n",
       "0  B-MISC O O O O B-LOC O O B-MISC O O O O O O O ...   \n",
       "1                              B-ORG O B-ORG I-ORG O   \n",
       "\n",
       "                                       entities_pred  llm_time  \\\n",
       "0  [{'entity': 'Egyptian government', 'label': 'O...  0.844970   \n",
       "1  [{'entity': 'Coritiba', 'label': 'ORG'}, {'ent...  0.696006   \n",
       "\n",
       "   llm_prompt_tokens  llm_completion_tokens  \n",
       "0                203                     62  \n",
       "1                171                     66  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>ner_tags_str</th>\n",
       "      <th>entities</th>\n",
       "      <th>llm_time</th>\n",
       "      <th>llm_prompt_tokens</th>\n",
       "      <th>llm_completion_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7567</td>\n",
       "      <td>Egyptian government newspapers have criticised...</td>\n",
       "      <td>B-MISC O O O O B-LOC O O B-MISC O O O O O O O ...</td>\n",
       "      <td>[{'entity': 'Egyptian', 'label': 'MISC'}, {'en...</td>\n",
       "      <td>0.844970</td>\n",
       "      <td>203</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>624</td>\n",
       "      <td>Coritiba 1 Atletico Mineiro 0</td>\n",
       "      <td>B-ORG O B-ORG I-ORG O</td>\n",
       "      <td>[{'entity': 'Coritiba', 'label': 'ORG'}, {'ent...</td>\n",
       "      <td>0.696006</td>\n",
       "      <td>171</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0    id  \\\n",
       "0             0           0  7567   \n",
       "1             1           1   624   \n",
       "\n",
       "                                       sentence_text  \\\n",
       "0  Egyptian government newspapers have criticised...   \n",
       "1                      Coritiba 1 Atletico Mineiro 0   \n",
       "\n",
       "                                        ner_tags_str  \\\n",
       "0  B-MISC O O O O B-LOC O O B-MISC O O O O O O O ...   \n",
       "1                              B-ORG O B-ORG I-ORG O   \n",
       "\n",
       "                                            entities  llm_time  \\\n",
       "0  [{'entity': 'Egyptian', 'label': 'MISC'}, {'en...  0.844970   \n",
       "1  [{'entity': 'Coritiba', 'label': 'ORG'}, {'ent...  0.696006   \n",
       "\n",
       "   llm_prompt_tokens  llm_completion_tokens  \n",
       "0                203                     62  \n",
       "1                171                     66  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_gemini_2_flash = pd.read_csv('ner_train_gemini.csv')\n",
    "df_golden = pd.read_csv('human_train.csv')\n",
    "\n",
    "df_gemini_2_flash = df_gemini_2_flash.drop(columns=['entities'])\n",
    "\n",
    "display(df_gemini_2_flash.head(2))\n",
    "display(df_golden.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e05ae73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>avg_time_sec</th>\n",
       "      <th>avg_cost_usd</th>\n",
       "      <th>count</th>\n",
       "      <th>final_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemini</td>\n",
       "      <td>0.884162</td>\n",
       "      <td>0.892988</td>\n",
       "      <td>0.887122</td>\n",
       "      <td>0.778604</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>1300</td>\n",
       "      <td>0.884162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model        f1  precision    recall  avg_time_sec  avg_cost_usd  count  \\\n",
       "0  gemini  0.884162   0.892988  0.887122      0.778604      0.000044   1300   \n",
       "\n",
       "   final_score  \n",
       "0     0.884162  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "price_dict = {\n",
    "    \"gemini\": {\n",
    "        \"prompt\": 0.1 / 1000000,\n",
    "        \"completion\": 0.4 / 1000000,\n",
    "    }\n",
    "}\n",
    "\n",
    "dfs = {\n",
    "    \"gemini\": df_gemini_2_flash,\n",
    "}\n",
    "\n",
    "results_df = run_comparison(dfs, df_golden, price_dict)\n",
    "display(results_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f99262",
   "metadata": {},
   "source": [
    "# prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb65684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_spacy_format(raw_data, nlp_blank):\n",
    "    training_data = []\n",
    "    \n",
    "    for text, annotations in raw_data:\n",
    "        doc = nlp_blank.make_doc(text)\n",
    "        spans = []\n",
    "        \n",
    "        for item in annotations:\n",
    "            entity_text = item[\"entity\"]\n",
    "            label = item[\"label\"]\n",
    "            \n",
    "            pattern_str = re.escape(entity_text)\n",
    "            pattern = re.compile(fr'(?<!\\w){pattern_str}(?!\\w)')\n",
    "            \n",
    "            for match in re.finditer(pattern, text):\n",
    "                start, end = match.span()\n",
    "                span = doc.char_span(start, end, label=label, alignment_mode=\"strict\")\n",
    "                \n",
    "                if span is None:\n",
    "                    print(f\"no match: {entity_text} in '{text}'\")\n",
    "                else:\n",
    "                    spans.append(span)\n",
    "        \n",
    "        doc.ents = filter_spans(spans)\n",
    "        \n",
    "        example = Example.from_dict(doc, {\"entities\": [(e.start_char, e.end_char, e.label_) for e in doc.ents]})\n",
    "        training_data.append(example)\n",
    "        \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d2bd9",
   "metadata": {},
   "source": [
    "# train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b30f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_english_model(training_examples, base_model=\"en_core_web_lg\", iterations=15):\n",
    "    print(f\"Loading base model: {base_model}...\")\n",
    "    try:\n",
    "        nlp = spacy.load(base_model)\n",
    "    except OSError:\n",
    "        print(f\"Model not found. Please run: python -m spacy download {base_model}\")\n",
    "        return None\n",
    "\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe(\"ner\", last=True)\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    for example in training_examples:\n",
    "        for ent in example.reference.ents:\n",
    "            ner.add_label(ent.label_)\n",
    "\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "\n",
    "    print(f\"Starting fine-tuning on {len(training_examples)} examples\")\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        optimizer = nlp.resume_training()\n",
    "        \n",
    "        for itn in range(iterations):\n",
    "            random.shuffle(training_examples)\n",
    "            losses = {}\n",
    "            \n",
    "            for example in training_examples:\n",
    "                nlp.update(\n",
    "                    [example],\n",
    "                    drop=0.3, \n",
    "                    sgd=optimizer,\n",
    "                    losses=losses,\n",
    "                )\n",
    "            \n",
    "            if (itn + 1) % 5 == 0:\n",
    "                print(f\"Epoch {itn + 1}, Loss: {losses['ner']:.4f}\")\n",
    "                \n",
    "    return nlp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909650cc",
   "metadata": {},
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdc3915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_to_df(nlp, examples):\n",
    "    scorer = Scorer()\n",
    "    examples_for_scoring = []\n",
    "    \n",
    "    for example in examples:\n",
    "        pred_doc = nlp(example.reference.text)\n",
    "        examples_for_scoring.append(Example(pred_doc, example.reference))\n",
    "    \n",
    "    scores = scorer.score(examples_for_scoring)\n",
    "    \n",
    "    metrics_data = []\n",
    "    metrics_data.append({\n",
    "        \"Label\": \"GLOBAL\",\n",
    "        \"Precision\": scores[\"ents_p\"], \n",
    "        \"Recall\": scores[\"ents_r\"], \n",
    "        \"F1-Score\": scores[\"ents_f\"]\n",
    "    })\n",
    "    \n",
    "    for label, metrics in scores[\"ents_per_type\"].items():\n",
    "        metrics_data.append({\n",
    "            \"Label\": label,\n",
    "            \"Precision\": metrics[\"p\"], \n",
    "            \"Recall\": metrics[\"r\"], \n",
    "            \"F1-Score\": metrics[\"f\"]\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(metrics_data).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73af0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(model: str, raw_data_train, raw_data_test):\n",
    "    try:\n",
    "        print(f'start train in {model}')\n",
    "        nlp_base = spacy.load(\"en_core_web_lg\")\n",
    "        \n",
    "        train_data = convert_to_spacy_format(raw_data_train, nlp_base)\n",
    "        test_data = convert_to_spacy_format(raw_data_test, nlp_base)\n",
    "        \n",
    "        nlp_finetuned = fine_tune_english_model(train_data, base_model=\"en_core_web_lg\", iterations=15)\n",
    "        \n",
    "        if nlp_finetuned:\n",
    "            print(\"\\nMetrics\")\n",
    "            df_metrics = evaluate_model_to_df(nlp_finetuned, test_data)\n",
    "            print(df_metrics)\n",
    "            \n",
    "            nlp_finetuned.to_disk(f\"./my_eng_ner_{model}\")\n",
    "\n",
    "    except OSError:\n",
    "        print(\"Error: 'en_core_web_lg' not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6122489",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv()\n",
    "test = pd.read_csv()\n",
    "\n",
    "train_spacy = list(zip(train['sentence_text'].values, train['entities'].values))\n",
    "test_spacy = list(zip(train['sentence_text'].values, train['entities'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4057f123",
   "metadata": {},
   "source": [
    "# RES\n",
    "\n",
    "## human\n",
    "- 15 it\n",
    "\n",
    "|index|Label|Precision|Recall|F1-Score|\n",
    "|---|---|---|---|---|\n",
    "|0|GLOBAL|0\\.853|0\\.861|0\\.857|\n",
    "|1|ORG|0\\.821|0\\.797|0\\.809|\n",
    "|2|MISC|0\\.756|0\\.798|0\\.776|\n",
    "|3|PER|0\\.899|0\\.895|0\\.897|\n",
    "|4|LOC|0\\.888|0\\.916|0\\.902|\n",
    "|5|DATE|0\\.0|0\\.0|0\\.0|\n",
    "\n",
    "## gemini \n",
    "- 15 it\n",
    "\n",
    "|index|Label|Precision|Recall|F1-Score|\n",
    "|---|---|---|---|---|\n",
    "|0|GLOBAL|0\\.750|0\\.744|0\\.747|\n",
    "|1|ORG|0\\.674|0\\.480|0\\.561|\n",
    "|2|MISC|0\\.665|0\\.577|0\\.618|\n",
    "|3|PER|0\\.843|0\\.913|0\\.876|\n",
    "|4|LOC|0\\.742|0\\.903|0\\.815|\n",
    "\n",
    "- 20 it\n",
    "\n",
    "|index|Label|Precision|Recall|F1-Score|\n",
    "|---|---|---|---|---|\n",
    "|0|GLOBAL|0\\.767|0\\.732|0\\.749|\n",
    "|1|ORG|0\\.676|0\\.51|0\\.581|\n",
    "|2|MISC|0\\.823|0\\.488|0\\.613|\n",
    "|3|PER|0\\.895|0\\.873|0\\.884|\n",
    "|4|LOC|0\\.712|0\\.915|0\\.801|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ddf159",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
