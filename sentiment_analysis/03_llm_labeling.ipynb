{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05a10c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a564799",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL_NAME = \"gpt-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118b639c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>mark</th>\n",
       "      <th>true_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6868</td>\n",
       "      <td>In Panic In The Streets Richard Widmark plays ...</td>\n",
       "      <td>8</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24016</td>\n",
       "      <td>If you ask me the first one was really better ...</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9668</td>\n",
       "      <td>I am a big fan a Faerie Tale Theatre and I've ...</td>\n",
       "      <td>10</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13640</td>\n",
       "      <td>I just finished reading a book about Dillinger...</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14018</td>\n",
       "      <td>Greg Davis and Bryan Daly take some crazed sta...</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>21575</td>\n",
       "      <td>My roommates &amp; I nearly shorted out our TV fro...</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>5390</td>\n",
       "      <td>Michelle Rodriguez is the defining actress who...</td>\n",
       "      <td>7</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>860</td>\n",
       "      <td>Nice movie with a great soundtrack which spans...</td>\n",
       "      <td>8</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>15795</td>\n",
       "      <td>Even though this was a made-for-TV production,...</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>23654</td>\n",
       "      <td>I saw this on cable recently and kinda enjoyed...</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                               text  mark  \\\n",
       "0       6868  In Panic In The Streets Richard Widmark plays ...     8   \n",
       "1      24016  If you ask me the first one was really better ...     1   \n",
       "2       9668  I am a big fan a Faerie Tale Theatre and I've ...    10   \n",
       "3      13640  I just finished reading a book about Dillinger...     1   \n",
       "4      14018  Greg Davis and Bryan Daly take some crazed sta...     2   \n",
       "...      ...                                                ...   ...   \n",
       "24995  21575  My roommates & I nearly shorted out our TV fro...     2   \n",
       "24996   5390  Michelle Rodriguez is the defining actress who...     7   \n",
       "24997    860  Nice movie with a great soundtrack which spans...     8   \n",
       "24998  15795  Even though this was a made-for-TV production,...     1   \n",
       "24999  23654  I saw this on cable recently and kinda enjoyed...     1   \n",
       "\n",
       "      true_label  \n",
       "0              P  \n",
       "1              N  \n",
       "2              P  \n",
       "3              N  \n",
       "4              N  \n",
       "...          ...  \n",
       "24995          N  \n",
       "24996          P  \n",
       "24997          P  \n",
       "24998          N  \n",
       "24999          N  \n",
       "\n",
       "[25000 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_pos_data = pd.read_csv('../data/amdb_dataset/train/train_pos_data.csv')\n",
    "train_neg_data = pd.read_csv('../data/amdb_dataset/train/train_neg_data.csv')\n",
    "test_pos_data = pd.read_csv('../data/amdb_dataset/test/test_pos_data.csv')\n",
    "test_neg_data = pd.read_csv('../data/amdb_dataset/test/test_neg_data.csv')\n",
    "\n",
    "train_pos_data['true_label'] = 'P'\n",
    "train_neg_data['true_label'] = 'N'\n",
    "test_pos_data['true_label'] = 'P'\n",
    "test_neg_data['true_label'] = 'N'\n",
    "\n",
    "train_df = pd.concat([train_pos_data, train_neg_data], ignore_index=True)\n",
    "test_df = pd.concat([test_pos_data, test_neg_data], ignore_index=True)\n",
    "    \n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04967640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_comment_with_prompt(comment, messages, model_name):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            temperature=0\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        pred = response['choices'][0]['message']['content'].strip()\n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "        prompt_tokens = response['usage']['prompt_tokens']\n",
    "        completion_tokens = response['usage']['completion_tokens']\n",
    "        \n",
    "        return pred, elapsed_time, prompt_tokens, completion_tokens\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при обработке комментария: {comment[:50]}... Ошибка: {e}\")\n",
    "        return \"error\", 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_dict = {\n",
    "    \"Friendly positive/negative\": (\n",
    "        \"You are a friendly assistant. Classify the following comment as Positive or Negative. \"\n",
    "        \"Now classify the comment:\\nComment: {text}\\nSentiment:\"\n",
    "    ),\n",
    "    \n",
    "    \"Strict formal\": (\n",
    "        \"You are a very strict and formal sentiment analyzer. Only label as Positive or Negative, no exceptions. \"\n",
    "        \"Examples:\\n\"\n",
    "        \"'Fantastic service!' -> Positive\\n\"\n",
    "        \"'I am extremely disappointed.' -> Negative\\n\\n\"\n",
    "        \"Classify the comment:\\n{text}\\nSentiment:\"\n",
    "    ),\n",
    "    \n",
    "    \"Emotional tone\": (\n",
    "        \"You are an emotional sentiment detector. Label the comment as Positive or Negative. \"\n",
    "        \"Examples:\\n\"\n",
    "        \"'I feel so happy and grateful today!' -> Positive\\n\"\n",
    "        \"'I am sad and frustrated with this.' -> Negative\\n\\n\"\n",
    "        \"Classify the comment:\\n{text}\\nSentiment:\"\n",
    "    ),\n",
    "    \n",
    "    \"Direct short\": (\n",
    "        \"Classify as Positive or Negative.\\n\"\n",
    "        \"Example: 'I hate this!' -> Negative\\n\"\n",
    "        \"Example: 'Love it!' -> Positive\\n\"\n",
    "        \"Comment: {text}\\nSentiment:\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa8434d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a40f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_prompts(prompts_dict, data_df, sample_size=50, model_name=MODEL_NAME):\n",
    "    results_list = []\n",
    "    sample_df = data_df.sample(n=min(sample_size, len(data_df)), random_state=42).copy()\n",
    "    \n",
    "    for prompt_name, prompt_template in prompts_dict.items():\n",
    "        sample_df[f'predicted_label_{prompt_name}'] = None\n",
    "        sample_df[f'time_sec_{prompt_name}'] = None\n",
    "        sample_df[f'prompt_tokens_{prompt_name}'] = None\n",
    "        sample_df[f'completion_tokens_{prompt_name}'] = None\n",
    "\n",
    "        all_preds = []\n",
    "        all_times = []\n",
    "        all_prompt_tokens = []\n",
    "        all_completion_tokens = []\n",
    "\n",
    "        for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=f\"Evaluating {prompt_name}\"):\n",
    "            comment = row['text']\n",
    "            \n",
    "            pred, elapsed_time, prompt_tokens, completion_tokens = classify_comment_with_prompt(\n",
    "                comment, prompt_template, model_name\n",
    "            )\n",
    "            \n",
    "            if \"positive\" in pred.lower():\n",
    "                pred = \"P\"\n",
    "            elif \"negative\" in pred.lower():\n",
    "                pred = \"N\"\n",
    "            else:\n",
    "                pred = \"None\"\n",
    "\n",
    "            all_preds.append(pred)\n",
    "            all_times.append(elapsed_time)\n",
    "            all_prompt_tokens.append(prompt_tokens)\n",
    "            all_completion_tokens.append(completion_tokens)\n",
    "\n",
    "        sample_df[f'predicted_label_{prompt_name}'] = all_preds\n",
    "        sample_df[f'time_sec_{prompt_name}'] = all_times\n",
    "        sample_df[f'prompt_tokens_{prompt_name}'] = all_prompt_tokens\n",
    "        sample_df[f'completion_tokens_{prompt_name}'] = all_completion_tokens\n",
    "\n",
    "        true_labels = sample_df['true_label'].tolist()\n",
    "        filtered_true = [true_labels[i] for i, pred in enumerate(all_preds) if pred in ['P', 'N']]\n",
    "        filtered_preds = [pred for pred in all_preds if pred in ['P', 'N']]\n",
    "\n",
    "        if len(filtered_true) == 0:\n",
    "            accuracy, precision, recall, f1 = 0, 0, 0, 0\n",
    "        else:\n",
    "            accuracy = accuracy_score(filtered_true, filtered_preds)\n",
    "            precision = precision_score(filtered_true, filtered_preds, pos_label='P', zero_division=0)\n",
    "            recall = recall_score(filtered_true, filtered_preds, pos_label='P', zero_division=0)\n",
    "            f1 = f1_score(filtered_true, filtered_preds, pos_label='P', zero_division=0)\n",
    "\n",
    "        avg_time = sum(all_times) / len(all_times) if all_times else 0\n",
    "\n",
    "        results_list.append({\n",
    "            'prompt_name': prompt_name,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'avg_time_per_comment': avg_time,\n",
    "            'total_prompt_tokens': sum(all_prompt_tokens),\n",
    "            'total_completion_tokens': sum(all_completion_tokens)\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    return results_df, sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e6560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_dataset(train_df, test_df, prompt, model_name=MODEL_NAME):\n",
    "    def process_df(df, df_name):\n",
    "        df = df.copy()\n",
    "        df['predicted_label'] = None\n",
    "        df['time_sec'] = None\n",
    "        df['prompt_tokens'] = None\n",
    "        df['completion_tokens'] = None\n",
    "\n",
    "        all_preds, all_times, all_prompt_tokens, all_completion_tokens = [], [], [], []\n",
    "\n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {df_name}\"):\n",
    "            comment = row['text']\n",
    "\n",
    "            pred, elapsed_time, prompt_tokens, completion_tokens = classify_comment_with_prompt(\n",
    "                comment, prompt, model_name\n",
    "            )\n",
    "\n",
    "            if \"positive\" in pred.lower():\n",
    "                pred = \"P\"\n",
    "            elif \"negative\" in pred.lower():\n",
    "                pred = \"N\"\n",
    "            else:\n",
    "                pred = \"None\"\n",
    "\n",
    "            all_preds.append(pred)\n",
    "            all_times.append(elapsed_time)\n",
    "            all_prompt_tokens.append(prompt_tokens)\n",
    "            all_completion_tokens.append(completion_tokens)\n",
    "\n",
    "        df['predicted_label'] = all_preds\n",
    "        df['time_sec'] = all_times\n",
    "        df['prompt_tokens'] = all_prompt_tokens\n",
    "        df['completion_tokens'] = all_completion_tokens\n",
    "\n",
    "        metrics = {\n",
    "            'avg_time_per_comment': sum(all_times)/len(all_times) if all_times else 0,\n",
    "            'total_prompt_tokens': sum(all_prompt_tokens),\n",
    "            'total_completion_tokens': sum(all_completion_tokens)\n",
    "        }\n",
    "\n",
    "        return df, metrics\n",
    "\n",
    "    processed_train_df, train_metrics = process_df(train_df, \"Train\")\n",
    "    processed_test_df, test_metrics = process_df(test_df, \"Test\")\n",
    "\n",
    "    processed_train_df.to_csv(\"train_with_predictions.csv\", index=False)\n",
    "    processed_test_df.to_csv(\"test_with_predictions.csv\", index=False)\n",
    "\n",
    "    return processed_train_df, processed_test_df, train_metrics, test_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
